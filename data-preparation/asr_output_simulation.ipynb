{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install speechbrain transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.inference.ASR import EncoderASR\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run inference on speechbrain ASR trained on morrocan dialect, using the test set of Devoice dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset could be obtained via this link : https://zenodo.org/records/6342622\n",
    "It contains a folder containing all the audio files of all sets, and another folder containing three csv files: test.csv, train.csv and dev.csv. each containing the filenames corressponding to that set. \n",
    "The first step was to extract the test set files from the folder containing all the audio files and copying them to another folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_filename(full_string):\n",
    "    # Split the string by whitespace or other separators to get parts ending with .wav\n",
    "    parts = full_string.split()\n",
    "    for part in parts:\n",
    "        if part.endswith('.wav'):\n",
    "            return part\n",
    "    return None\n",
    "\n",
    "def copy_files(csv_file, source_folder, target_folder):\n",
    "    with open(csv_file, 'r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        for row in csv_reader:\n",
    "            if row: \n",
    "                full_string = row[0]  #file names are in the first column\n",
    "                filename = extract_filename(full_string)\n",
    "                \n",
    "                if filename:\n",
    "                    source_path = os.path.join(source_folder, filename)\n",
    "                    target_path = os.path.join(target_folder, filename)\n",
    "                    \n",
    "                    try:\n",
    "                        shutil.copy(source_path, target_path)\n",
    "                        print(f\"Successfully copied {filename} to {target_folder}\")\n",
    "                    except FileNotFoundError:\n",
    "                        print(f\"File {filename} not found in {source_folder}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error copying {filename}: {str(e)}\")\n",
    "                else:\n",
    "                    print(f\"No .wav file found in '{full_string}'\")\n",
    "\n",
    "\n",
    "csv_file = 'path_to_test_csv' \n",
    "source_folder = 'path_to_the_folder_containing_audio_files'\n",
    "target_folder = 'target where to copy the test audio files'   \n",
    "\n",
    "copy_files(csv_file, source_folder, target_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ASR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model = EncoderASR.from_hparams(source=\"speechbrain/asr-wav2vec2-dvoice-darija\", savedir=\"pretrained_models/asr-wav2vec2-dvoice-darija\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run inference on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_with_asr(csv_file, source_folder, target_folder):\n",
    "    transcriptions = []\n",
    "    file_name = []\n",
    "    with open(csv_file, 'r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        for row in csv_reader:\n",
    "            if row:  \n",
    "                full_string = row[0] \n",
    "                filename = extract_filename(full_string)\n",
    "                if filename:\n",
    "                    fullpath = os.path.join(source_folder, filename)\n",
    "                    output = asr_model.transcribe_file(full_path)\n",
    "                    transcriptions.append(output)\n",
    "                    file_name.append(file)\n",
    "    df = pd.DataFrame({'file_names': file_name, 'transcriptions': transcriptions})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the ASR output is analysed, we selected these errors patterns (common with most of asr models):\n",
    "-missing words\n",
    "-missing character\n",
    "-repeating words\n",
    "-false word recognition\n",
    "So in order to simulate these errors, we created these function that introduce these errors patterns to an input sentence. For false word recognition, a random word is selected, and replaced by a random combination of arabic chars "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_chars_list = ['ء', 'آ', 'أ', 'ؤ', 'إ', 'ئ', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ى', 'ي']\n",
    "\n",
    "\n",
    "def remove_random_space(sentence):\n",
    "    if ' ' in sentence:\n",
    "        index = random.choice([i for i, char in enumerate(sentence) if char == ' '])\n",
    "        return sentence[:index] + sentence[index+1:]\n",
    "    return sentence\n",
    "\n",
    "def remove_random_words(sentence):\n",
    "    words = sentence.split()\n",
    "    if len(words) > 2:\n",
    "        num_words_to_remove = random.randint(1, 2)\n",
    "        words_to_remove = random.sample(words, num_words_to_remove)\n",
    "        modified_sentence = ' '.join([word for word in words if word not in words_to_remove])\n",
    "        return modified_sentence\n",
    "    else:\n",
    "        return sentence\n",
    "\n",
    "def remove_random_characters(sentence):\n",
    "    words = sentence.split()\n",
    "    if words:\n",
    "        word_to_modify = random.choice(words)\n",
    "        if len(word_to_modify) > 1:\n",
    "            index_to_modify = random.randint(0, len(word_to_modify)-1)\n",
    "            modified_word = word_to_modify[:index_to_modify] + word_to_modify[index_to_modify+1:]\n",
    "            modified_sentence = ' '.join([modified_word if word == word_to_modify else word for word in words])\n",
    "            return modified_sentence\n",
    "    return sentence\n",
    "\n",
    "def repeat_random_word(sentence):\n",
    "    words = sentence.split()\n",
    "    if words:\n",
    "        word_to_repeat = random.choice(words)\n",
    "        index_to_repeat = words.index(word_to_repeat)\n",
    "        words.insert(index_to_repeat + 1, word_to_repeat)\n",
    "        modified_sentence = ' '.join(words)\n",
    "        return modified_sentence\n",
    "    return sentence\n",
    "\n",
    "def replace_with_random_chars(sentence):\n",
    "    words = sentence.split()\n",
    "    if words:\n",
    "        word_to_replace = random.choice(words)\n",
    "        num_chars = len(word_to_replace)\n",
    "        num_chars_to_replace = num_chars // 2\n",
    "        arabic_replacements = ''.join(random.choices(arabic_chars_list, k=num_chars_to_replace))\n",
    "        replaced_word = arabic_replacements + word_to_replace[num_chars_to_replace:]\n",
    "        modified_sentence = ' '.join([replaced_word if word == word_to_replace else word for word in words])\n",
    "        return modified_sentence\n",
    "    return sentence\n",
    "\n",
    "def apply_random_functions(sentence):\n",
    "    functions = [replace_with_random_chars , remove_random_space, remove_random_words, repeat_random_word, remove_random_characters]\n",
    "    selected_functions = random.sample(functions, 3)\n",
    "    for func in selected_functions:\n",
    "        sentence = func(sentence)\n",
    "    return sentence\n",
    "\n",
    "def split_and_apply(sentence):\n",
    "    sentences = re.split(r'[.!?,:]', sentence)\n",
    "    modified_sentences = []\n",
    "    for sent in sentences:\n",
    "        if sent.strip():\n",
    "            modified_sentence = apply_random_functions(sent.strip())\n",
    "            modified_sentences.append(modified_sentence)\n",
    "    return '. '.join(modified_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"هذه جملة باللغة العربية. وهذه جملة أخرى تتبعها. وهذه ثالثة، تحتوي على كلمات متعددة. وأخيراً، هذه الجملة الأخيرة في النص.\"\n",
    "modified_text = split_and_apply(input_text)\n",
    "print(modified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morrocan_summarization_dataset = pd.read_csv(\"./darija19k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_like_outpt = []\n",
    "for index, row in morrocan_summarization_dataset.iterrows():\n",
    "    asr_like_outpt.append(split_and_apply(row[\"Text\"]))\n",
    "morrocan_summarization_dataset[\"asr_like_ouput\"] = asr_like_outpt"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
