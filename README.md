## Darija Summarization
- [Description](#Description)
- [Usage](#Usage)
- [Dataset](#Dataset_preparation)
- [Evaluation](#Evaluation)


## Description 
This application was developed during the Mistral AI fine-tuning hackathon to tackle the issue of inaccurate text output from models like Automatic Speech Recognition (ASR) and Optical Character Recognition (OCR), particularly for the Moroccan dialect (Darija), which is considered a low-resource language. The project integrates a language model designed to understand the context of inaccurately transcribed text, enabling it to extract meaningful insights and generate summaries.


## Usage

# Installation 
```bash
pip install -r requirements.txt
```

## Setting Up Environment Variables

Before running the application, you need to set up the following environment variables:

- `MISTRAL_API_KEY`: Your API key for the Mistral client.
- `MISTRAL_JOB_ID`: The job ID for the fine-tuned model you want to use.
- `WAV2VEC2_TOKENIZER_PATH`: The path to the tokenizer file for the Wav2Vec2 model.

You can set these variables in your terminal or add them to a `.env` file.

### Setting Environment Variables in the Terminal

```sh
export MISTRAL_API_KEY='your_mistral_api_key_here'
export MISTRAL_JOB_ID='your_mistral_job_id_here'
export WAV2VEC2_TOKENIZER_PATH='your_wav2vec2_tokenizer_path_here'
```
# Start the demo: 

```bash
streamlit run app.py
```


## Fine-tuning details 
As part of the hackathon, we fine-tuned the Mistral model specifically for summarizing inaccurately transformed text using Mistral APIs. The fine-tuned model used was "mistral-open-7b".


## Dataset preparation 
To prepare our dataset for summarizing Moroccan Darija dialect text, we started with the Goud dataset, which contains articles in Moroccan Darija. From this dataset, we selected 19,000 rows consisting primarily of Moroccan text with a small amount of Modern Standard Arabic.

To simulate realistic errors that could occur in text transformation models, we adopted two approaches:

1-Augmenting data with errors generated by an OCR model: This involved transforming input text into images with compressed, small text and applying a blurring effect. The resulting images were processed through an OCR model to produce text with genuine errors.

2-Augmenting text with ASR-like errors: We used a Maghrebian ASR model to generate transcription on various maghrebian audio files. Patterns detected by the ASR model were then incorporated into the data via a Python script.

This augmentation process resulted in a dataset expanded to 38,000 rows (19,000 original rows multiplied by 2 augmentation methods), with a total Word Error Rate (WER) score distributed accordingly.

| Transformed text             | CER           | WER|
|------------------------------|-----------------|-----------|
| OCR output |  0.183 | 0.65     |
| ASR simulated output | |  |
|Total  |  |    |


For detailed information on dataset creation scripts, visit [URL].

## Evaluation 
To evaluate our models, we utilized the ROUGE metric, comparing the output summaries against ground truth summaries. We established two baselines for comparison:

    1-The Goud model trained on 154,000 summaries in Darija [URL], evaluated on OCR outputs and augmented error data.

    2-Zero-shot performance of the Mistral model using OCR outputs and augmented error data.

Detailed results are illustrated in the accompanying table.

| Model                         | Input           | Rouge-1-F | Rouge-2-F | Rouge-l-F |
|-------------------------------|-----------------|-----------|-----------|-----------|
| DarijaBERT-summarization-goud | Original text   | 0.116     | 0.0404    | 0.109     |
| Mistral_large                 | Transormed text | 0.057     | 0.012     | 0.055     |
| Fine-tuned-open-mistral-7b    | Transormed text | 0.086     | 0.008     | 0.081     |

